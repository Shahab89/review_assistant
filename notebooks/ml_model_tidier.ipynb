{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = \"Given this product title, please select between 3 and 6 criteria to rate in order to compose a product review. No need to explain the criteria.\"\n",
    "input_product = \"Nike Men's Revolution 5 Running Shoes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "def get_text_chunks(text) -> list[Document]:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    docs = [Document(page_content=x) for x in text_splitter.split_text(text)]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Nike Men's Revolution 5 Running Shoes\")]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = get_text_chunks(input_product)\n",
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains.retrieval_qa.base import BaseRetrievalQA\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "OPENAI_MODEL = os.environ.get('OPENAI_MODEL') # 'gpt-3.5-turbo'\n",
    "BASE_PERSIST_DIRECTORY = '../db/chroma_3/'\n",
    "\n",
    "\n",
    "def embed_texts(texts, model_name) -> list[Document]:\n",
    "    if model_name == 'openai':\n",
    "        embedder = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "    else:\n",
    "        embedder = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "    vector_db = Chroma.from_documents(\n",
    "        texts,\n",
    "        embedder,\n",
    "        persist_directory=BASE_PERSIST_DIRECTORY + model_name\n",
    "    )\n",
    "    return vector_db\n",
    "\n",
    "def get_qa(model, vector_db) -> BaseRetrievalQA:\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=model,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vector_db.as_retriever(search_kwargs={\"k\": 1}),\n",
    "    return_source_documents=False,\n",
    "    verbose=False,\n",
    "    )\n",
    "    return qa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.Completions object at 0x7f168b4bb7c0> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f168b4ecf10> openai_api_key=SecretStr('**********') openai_proxy='' \n",
      "\n",
      "combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=ChatPromptTemplate(input_variables=['context', 'question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7f168b4bb7c0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f168b4ecf10>, openai_api_key=SecretStr('**********'), openai_proxy='')), document_variable_name='context') retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7f169b0fdcf0>, search_kwargs={'k': 1}) \n",
      "\n",
      "1. Comfort\n",
      "2. Durability\n",
      "3. Fit\n",
      "4. Support\n",
      "5. Style\n",
      "6. Performance\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm_openai = ChatOpenAI(model_name=OPENAI_MODEL, openai_api_key=OPENAI_API_KEY)\n",
    "print(llm_openai, '\\n')\n",
    "\n",
    "vector_db = embed_texts(texts, model_name='openai')\n",
    "qa_openai = get_qa(llm_openai, vector_db)\n",
    "print(qa_openai, '\\n')\n",
    "\n",
    "answer = qa_openai.invoke(prompt_text)\n",
    "print(answer[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Resuming transfer from byte position 1979946720\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 27242    0 27242    0     0  58433      0 --:--:-- --:--:-- --:--:-- 58334\n"
     ]
    }
   ],
   "source": [
    "HG_EMBEDDING_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "#MODEL_NAME = 'mistral-7b-openorca.gguf2.Q4_0.gguf'  # GPT4All model\n",
    "MODEL_NAME = 'orca-mini-3b-gguf2-q4_0.gguf'\n",
    "MODEL_PATH = '../models/' + MODEL_NAME\n",
    "!curl -C - -o {MODEL_PATH} https://gpt4all.io/models/gguf/{MODEL_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to load libllamamodel-mainline-cuda.so: dlopen: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "Failed to load libllamamodel-mainline-cuda-avxonly.so: dlopen: libcuda.so.1: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm_local----->   \u001b[1mGPT4All\u001b[0m\n",
      "Params: {'model': '../models/orca-mini-3b-gguf2-q4_0.gguf', 'max_tokens': 200, 'n_predict': 256, 'top_k': 40, 'top_p': 0.1, 'temp': 0.7, 'n_batch': 8, 'repeat_penalty': 1.18, 'repeat_last_n': 64, 'streaming': False} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shahab/.pyenv/versions/3.10.6/envs/review_env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/home/shahab/.pyenv/versions/3.10.6/envs/review_env/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/shahab/.pyenv/versions/3.10.6/envs/review_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qa_local----->   combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"), llm=GPT4All(verbose=True, callbacks=[<langchain.callbacks.streaming_stdout_final_only.FinalStreamingStdOutCallbackHandler object at 0x7f169b0ffc10>], model='../models/orca-mini-3b-gguf2-q4_0.gguf', client=<gpt4all.gpt4all.GPT4All object at 0x7f169b0ff340>)), document_variable_name='context') retriever=VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7f151c6b3d60>, search_kwargs={'k': 1}) \n",
      "\n",
      "answer----->    4 criteria (e.g., comfort, durability, style, performance)\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.streaming_stdout_final_only import FinalStreamingStdOutCallbackHandler\n",
    "from langchain.llms import GPT4All\n",
    "\n",
    "callbacks = [FinalStreamingStdOutCallbackHandler()]\n",
    "llm_local =  GPT4All(model=MODEL_PATH, callbacks=callbacks, verbose=True)\n",
    "print('llm_local----->  ', llm_local, '\\n')\n",
    "\n",
    "vector_db =  embed_texts(texts, model_name=HG_EMBEDDING_MODEL)\n",
    "qa_local = get_qa(llm_local, vector_db)\n",
    "print('qa_local----->  ', qa_local, '\\n')\n",
    "\n",
    "answer =  qa_local.invoke(prompt_text)\n",
    "print('answer----->  ',answer[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt--->  input_variables=['product_type'] template=\"Product: '{product_type}'\\nPlease list between 3 and 6 criteria that could be rated for this product. Just return the list of criteria, nothing more.\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_text2 = \"Please list between 3 and 6 criteria that could be rated for this product. Just return the list of criteria, nothing more.\"\n",
    "#template = f\"\"\"Product: '{{product_type}}'\\n{prompt_text2}\"\"\".format(product_type=input_product)\n",
    "#prompt = PromptTemplate(template=template, input_variables=[\"product_type\"])\n",
    "\n",
    "\n",
    "\n",
    "template = f\"\"\"Product: '{{product_type}}'\n",
    "{prompt_text2}\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"product_type\"])\n",
    "\n",
    "print('prompt---> ', prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm_chain--->  prompt=PromptTemplate(input_variables=['product_type'], template=\"Product: '{product_type}'\\nPlease list between 3 and 6 criteria that could be rated for this product. Just return the list of criteria, nothing more.\") llm=GPT4All(verbose=True, callbacks=[<langchain.callbacks.streaming_stdout_final_only.FinalStreamingStdOutCallbackHandler object at 0x7f169b0ffc10>], model='../models/orca-mini-3b-gguf2-q4_0.gguf', client=<gpt4all.gpt4all.GPT4All object at 0x7f169b0ff340>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm_local, return_final_only=True)\n",
    "llm_chain\n",
    "print('llm_chain---> ', llm_chain, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'product_type': \"Nike Men's Revolution 5 Running Shoes\", 'text': ''}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res =  llm_chain.invoke(input_product)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n1. Comfort level\\n2. Durability\\n3. Breathability\\n4. Cushioning\\n5. Traction\\n6. Design/style'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(openai_api_key=OPENAI_API_KEY)\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, return_final_only=True)\n",
    "res_openai = llm_chain.invoke(input_product)\n",
    "result = res_openai['text']\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "review_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
